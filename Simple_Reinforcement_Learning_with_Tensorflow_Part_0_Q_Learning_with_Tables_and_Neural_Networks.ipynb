{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "o3Pj-nSIkgk-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 4
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c32b3cd4-a7eb-453c-ca0f-28207b02b71f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519824374052,
          "user_tz": -180,
          "elapsed": 15483,
          "user": {
            "displayName": "Goktug Enes GOKTURK",
            "photoUrl": "//lh4.googleusercontent.com/-lyEwLGonbe0/AAAAAAAAAAI/AAAAAAAAABY/F_8GdchDz1k/s50-c-k-no/photo.jpg",
            "userId": "116111353413070423287"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"For this application,\n",
        "\n",
        "\"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0 \" \n",
        "the link(above) used as a reference. \n",
        "\n",
        "\"\"\"\n",
        "#To make some adjustments\n",
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9fRGpXimlnUS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_5zP-TrNmDos",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'drive/Colab Notebooks') #add your google colab files path\n",
        "#Finished the adjustments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sohDvzA1mJ-t",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32bb22b4-db65-4b40-c1cf-9b1739ae10de",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519824614942,
          "user_tz": -180,
          "elapsed": 3588,
          "user": {
            "displayName": "Goktug Enes GOKTURK",
            "photoUrl": "//lh4.googleusercontent.com/-lyEwLGonbe0/AAAAAAAAAAI/AAAAAAAAABY/F_8GdchDz1k/s50-c-k-no/photo.jpg",
            "userId": "116111353413070423287"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Let's learn where we are working on (CPU or GPU)\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "#İf you are working on CPU, You're gonna see that answer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "aE_O0Z6tmyfT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f19b962c-52d8-4f4f-95f9-d5ad208b4f35",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519824729654,
          "user_tz": -180,
          "elapsed": 2232,
          "user": {
            "displayName": "Goktug Enes GOKTURK",
            "photoUrl": "//lh4.googleusercontent.com/-lyEwLGonbe0/AAAAAAAAAAI/AAAAAAAAABY/F_8GdchDz1k/s50-c-k-no/photo.jpg",
            "userId": "116111353413070423287"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "#İt shows we are working on GPU"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "htHE1Zdmq3_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. While both approaches ultimately allow us to take intelligent actions given a situation, the means of getting to that action differ significantly [link text](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)"
      ]
    },
    {
      "metadata": {
        "id": "0Qys_ZRInLHt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ol7RvE6TteZC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "env=gym.make('FrozenLake-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XIMZ05xbtnCJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Initialize table with all zeros\n",
        "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
        "# Set learning parameters\n",
        "lr = .8\n",
        "y = .95\n",
        "num_episodes = 2000\n",
        "#create lists to contain total rewards and steps per episode\n",
        "#jList = []\n",
        "rList = []\n",
        "for i in range(num_episodes):\n",
        "    #Reset environment and get first new observation\n",
        "    s = env.reset()\n",
        "    rAll = 0\n",
        "    d = False\n",
        "    j = 0\n",
        "    #The Q-Table learning algorithm\n",
        "    while j < 99:\n",
        "        j+=1\n",
        "        #Choose an action by greedily (with noise) picking from Q table\n",
        "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
        "        #Get new state and reward from environment\n",
        "        s1,r,d,_ = env.step(a)\n",
        "        #Update Q-Table with new knowledge\n",
        "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
        "        rAll += r\n",
        "        s = s1\n",
        "        if d == True:\n",
        "            break\n",
        "    #jList.append(j)\n",
        "    rList.append(rAll)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0U_fgoIOyFg_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90b72ead-ea1d-4084-bac2-716ed043ef31",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519828213505,
          "user_tz": -180,
          "elapsed": 677,
          "user": {
            "displayName": "Goktug Enes GOKTURK",
            "photoUrl": "//lh4.googleusercontent.com/-lyEwLGonbe0/AAAAAAAAAAI/AAAAAAAAABY/F_8GdchDz1k/s50-c-k-no/photo.jpg",
            "userId": "116111353413070423287"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (\"Score over time: \" +  str(sum(rList)/num_episodes))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 0.348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PAL3Ig2Uy1EH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8415d2bd-68a2-4e29-a59a-3cb94391ffa0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519828235855,
          "user_tz": -180,
          "elapsed": 707,
          "user": {
            "displayName": "Goktug Enes GOKTURK",
            "photoUrl": "//lh4.googleusercontent.com/-lyEwLGonbe0/AAAAAAAAAAI/AAAAAAAAABY/F_8GdchDz1k/s50-c-k-no/photo.jpg",
            "userId": "116111353413070423287"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (\"Final Q-Table Values\")\n",
        "print (Q)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Q-Table Values\n",
            "[[3.46664620e-03 6.63966596e-02 2.57931413e-03 3.47844773e-03]\n",
            " [1.17742074e-04 1.65354677e-04 2.74433422e-04 1.93584694e-02]\n",
            " [9.98325442e-02 1.34716444e-03 6.84098839e-04 1.42643040e-03]\n",
            " [3.44161966e-04 5.59195484e-05 6.97290931e-05 1.34406671e-03]\n",
            " [8.64021185e-02 3.01733060e-03 2.59426228e-03 3.63885119e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.59880022e-01 2.24190875e-11 1.24383898e-03 6.70144281e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.03186344e-03 0.00000000e+00 6.39543659e-04 3.02420335e-01]\n",
            " [4.06189766e-04 6.68995773e-01 2.45018699e-03 8.16106138e-04]\n",
            " [8.99270949e-01 2.27226545e-04 0.00000000e+00 1.03452172e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.07253358e-03 7.11262930e-04 3.22567873e-01 1.75792945e-04]\n",
            " [0.00000000e+00 0.00000000e+00 9.94691919e-01 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FilMsfqe0nNH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}